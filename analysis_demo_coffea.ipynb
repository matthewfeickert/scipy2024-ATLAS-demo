{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the Scientific Python ecosystem helps answering fundamental questions of the Universe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LHC and the ATLAS experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATLAS experiment at CERN explores vast amounts of physics data to answer the most fundamental questions of the Universe.\n",
    "\n",
    "* What is Dark Matter, and can we produce dark matter particles in the LHC?\n",
    "* Are there extra spatial dimensions beyond the familiar three?\n",
    "* Why is there more matter than antimatter in the universe?\n",
    "* Are there new fundamental particles or forces?\n",
    "* ...\n",
    "\n",
    "The Large Hadron Collider (LHC) is a circular particle accelerator that provides high energy proton-proton collisions (_events_), from which new particles are being produced due to the basic equation that relates energy and matter:  \n",
    "$$E = mc^2$$\n",
    "\n",
    "![](figures/LHC_collisions.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ATLAS detector is mounted in one of the four interaction points of the LHC -- other detectors are mounted in other interaction points.\n",
    "\n",
    "![](figures/LHC.png)\n",
    "\n",
    "Those outgoing, newly produced particles, leave signatures in the ATLAS detector. Physicists reconstruct particle objects from those signatures.\n",
    "\n",
    "ATLAS is a complex detector, composed by various sub-detectors, each one specialized to the detection of a specific signature/particle.\n",
    "\n",
    "![](figures/ATLASImage.jpg)\n",
    "\n",
    "![](figures/Schematic-of-how-different-particles-interact-with-the-ATLAS-detector.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ATLAS physics data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those reconstructed particle objects, after various processing and data reduction steps, are stored in a compact data format called [PHYSLITE](https://opendata.atlas.cern/docs/documentation/data_format/physlite/) ($\\sim 10\\,\\text{kB/event}$). Physicists analyze PHYSLITE data to study the properties of the particles produced in each event.\n",
    "\n",
    "PHYSLITE data are highly structured and can be represented in a tabular format. However, since each event can contain variable number of particles, this will be a _jagged_ table. Learn more about [jagged data](https://en.wikipedia.org/wiki/Jagged_array) in the Part 3 of the tutorial [Thinking In Arrays](https://github.com/ekourlit/scipy2024-tutorial-thinking-in-arrays).\n",
    "\n",
    "There is a specialized Python library that provides NumPy-like idioms for arbitrary data structures that we will utilize. The [`awkward-array`](https://github.com/scikit-hep/awkward). \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "example = ak.Array([\n",
    "    [{\"x\": 1.1, \"y\": 1.2, \"z\": 3.1}, {\"x\": 2.2, \"y\": 1.3, \"z\": 2}, {\"x\": 3.3, \"y\": 2.4, \"z\": 4.2}],     # this event contains three electrons with properties x, y, and z\n",
    "    [],                                                                                                 # this event contains no electrons\n",
    "    [{\"x\": 4.4, \"y\": 1.1, \"z\": 1}, {\"x\": 5.5, \"y\": 4.2, \"z\": 3.2}]                                      # this event contains two electrons with properties x, y, and z\n",
    "])\n",
    "\n",
    "ak.to_dataframe(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is well defined are the properties of the particles we store in PHYSLITE files. Those are all listed in:\n",
    "\n",
    "https://atlas-physlite-content-opendata.web.cern.ch/\n",
    "\n",
    "ATLAS has recently released 65 TB of PHYSLITE open data for research -- this is over 7 billion LHC collision events! Those are all the data collected by the experiment during the 2015 and 2016. The release is accompanied by additional 2 billion events of simulated “Monte Carlo” data, which are essential for carrying out a physics analysis. The simulated data have largely the same structure as the real data. We're going to use these simulated events for today's demonstration for practical purposes.\n",
    "\n",
    "Read about our open data release at: \n",
    "\n",
    "https://atlas.cern/Updates/News/Open-Data-Research\n",
    "\n",
    "Our open data portal provides in depth information about the data along with analysis tutorials:\n",
    "\n",
    "https://opendata.atlas.cern/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading PHYSLITE files\n",
    "\n",
    "PHYSLITE files are of type [ROOT](https://root.cern/) files. This is a very common file type used in particle physics and needs specialized decompression and interpretation routines to load the data in memory (e.g. awkward arrays). For such purpose we will use the [`uproot`](https://github.com/scikit-hep/uproot5) library.\n",
    "\n",
    "So now we have introduced the two main libraries we utilize for particle physics data analysis in the Scientific Python ecosystem:\n",
    "1. `awkward`\n",
    "2. `uproot`\n",
    "\n",
    "Those two libraries are in principle enough for a physicist to acquire an in-memory representation of the data. However, to make our life easier we will use an additional library which is build on top of `awkward` and `uproot`. This is called [`coffea`](https://github.com/CoffeaTeam/coffea) and provides basic tools and wrappers for all the typical needs of particle physics data analysts who use the Scientific Python ecosystem.\n",
    "\n",
    "Let's import some required classes and functions from `coffea` along with `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coffea\n",
    "print(\"coffea version: \",coffea.__version__)\n",
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from coffea import dataset_tools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The particular analysis case we will use for this demonstration is the search for the Higgs boson, an elementary particle discovered in 2012. The Higgs boson was predicted by the BEH mechanism, the mechanism that explains how all the particles in the Universe acquire mass. The discovery of the Higgs boson by the ATLAS and CMS experiments at CERN awarded the Nobel prize for physics to Francois Englert and Peter W. Higgs in 2013.\n",
    "\n",
    "![](figures/nobel_2013.jpg)\n",
    "\n",
    "Read more about this landmark discovery here:\n",
    "\n",
    "https://atlas.cern/Discover/Physics/Higgs\n",
    "\n",
    "The Higgs boson is produced by the proton-proton collisions of the LHC. However, it is extremely short lived (about $10^{-22}$ seconds) and immediately decays to other particles. For this scenario we will study the decays of the Higgs boson to two other elementary particles, the Z bosons. In their turn, the Z bosons are extremely short lived as well. However, at about 3% of the time, they decay to two electrons. Electrons are also elementary particle but stable ones! Therefore we will look at our detector for electron signatures. In particular four electrons.\n",
    "\n",
    "![](figures/fig_01d.png)\n",
    "\n",
    "This is an actual four electron signature registered at the ATLAS detector in 2012 and contributed to the Higgs boson discovery:\n",
    "\n",
    "![](figures/higgs_event_display.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load a PHYSLITE file containing simulated event data describing the production and decay of Higgs bosons. Those data can be found at DOI:[10.7483/OPENDATA.ATLAS.Z2J9.709J](http://doi.org/10.7483/OPENDATA.ATLAS.Z2J9.709J) and will be streamed using the [XCache](http://slateci.io/XCache/) service. You can always download those data locally at your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hgg -> 4l sample\n",
    "\n",
    "# local\n",
    "file_path = '/Users/ekourlitis/cernbox/mc20_13TeV.345060.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_ZZ4l.deriv.DAOD_PHYSLITE.e7735_s3681_r13167_p6026/mc20_13TeV/DAOD_PHYSLITE.38191712._000020.pool.root.1'\n",
    "\n",
    "# UChicago\n",
    "# file_path = 'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000001.pool.root.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the loading lighter, we will use the `filter_name` function, in order to load only the properties needed. Then we will use `coffea`'s `NanoEventsFactory.from_root` to load the file. This is actually a delayed computation. `coffea` is using `dask` as a backend to construct a delayed task graph representing our analysis. So there is some metadata reading but not actual number crunching until `compute()` is called.\n",
    "\n",
    "Some warnings will appear as we're still working with `uproot` to correctly interpret all the properties stored in PHYSLITE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_name(name):\n",
    "    return name in [\n",
    "        \"EventInfo.mcEventWeights\",\n",
    "        \"AnalysisElectronsAuxDyn.pt\",\n",
    "        \"AnalysisElectronsAuxDyn.eta\",\n",
    "        \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\",\n",
    "        \"AnalysisElectronsAuxDyn.DFCommonElectronsLHLoose\",\n",
    "        \"AnalysisElectronsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVALooseCone_pt1000\",\n",
    "        \"AnalysisElectronsAuxDyn.topoetcone20\",\n",
    "        \"AnalysisElectronsAuxDyn.charge\",\n",
    "        \"AnalysisMuonsAuxDyn.pt\",\n",
    "        \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\",\n",
    "        \"AnalysisMuonsAuxDyn.m\",\n",
    "        \"AnalysisMuonsAuxDyn.quality\",\n",
    "        \"AnalysisMuonsAuxDyn.ptvarcone30_Nonprompt_All_MaxWeightTTVA_pt1000\",\n",
    "        \"AnalysisMuonsAuxDyn.topoetcone20\",\n",
    "    ]\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {file_path: \"CollectionTree\"}, # all the event properties are stored in the so called CollectionTree\n",
    "    schemaclass=PHYSLITESchema, # tell NanoEventsFactory.from_root that you read a PHYSLITE file\n",
    "    uproot_options=dict(filter_name=filter_name)\n",
    ").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highly structured jagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.Electrons.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection and feature engineering\n",
    "\n",
    "The data in every event of that simulated sample come from Higgs boson that decay to Z bosons. In particular, the Z boson can also decay to another elementary particle called muon -- thus we could alternatively look for four muon events -- but we will neglect those cases.\n",
    "\n",
    "Furthermore, we always apply some kinematic and quality criteria to our reconstructed particles (electrons and muons). Those criteria are defined by the `object_selection` function. After we get our _good_ particles definition, we will use them to only select events that fullfil those requirements for further processing. This is achieved by the `region_selection` function.\n",
    "\n",
    "Last but not least, we want to use the properties of the four electrons to deduct a critical property of the Higgs boson, its mass. In particular, we will calculate the so-called \"invariant\" mass, which is an inferred mass of a particle (Higgs) from its decay products (electrons). For this we use the function `calculate_inv_mass`. Read more about the invariant mass here: https://atlas.cern/glossary/mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GeV = 1000\n",
    "\n",
    "def object_selection(events):\n",
    "    '''\n",
    "    Select objects based on kinematic and quality criteria\n",
    "    '''\n",
    "    \n",
    "    electrons = events.Electrons\n",
    "    muons = events.Muons\n",
    "    \n",
    "    electron_reqs = (electrons.pt/GeV > 20) & \\\n",
    "                    (np.abs(electrons.eta) < 2.47) & \\\n",
    "                    (electrons.DFCommonElectronsLHLoose == 1)\n",
    "                    \n",
    "    muon_reqs = (muons.pt/GeV > 20) & \\\n",
    "                (np.abs(muons.eta) < 2.7) & \\\n",
    "                (muons.quality == 2)\n",
    "    \n",
    "    # only keep objects that pass our requirements\n",
    "    electrons = electrons[electron_reqs]\n",
    "    muons = muons[muon_reqs]\n",
    "    \n",
    "    return electrons, muons\n",
    "\n",
    "def region_selection(electrons, muons):\n",
    "    '''\n",
    "    Select events based on object multiplicity\n",
    "    '''\n",
    "    \n",
    "    selections = PackedSelection(dtype='uint64')\n",
    "    # basic selection criteria\n",
    "    selections.add(\"exactly_4e\", ak.num(electrons) == 4)\n",
    "    selections.add(\"total_e_charge_zero\", ak.sum(electrons.charge, axis=1) == 0)\n",
    "    selections.add(\"exactly_0m\", ak.num(muons) == 0)\n",
    "    # selection criteria combination\n",
    "    selections.add(\"4e0m\", selections.all(\"exactly_4e\", \"total_e_charge_zero\", \"exactly_0m\"))\n",
    "    \n",
    "    return selections.all(\"4e0m\")\n",
    "\n",
    "def calculate_inv_mass(electrons):\n",
    "    '''\n",
    "    Construct invariant mass observable\n",
    "    '''\n",
    "    \n",
    "    # reconstruct Higgs as 4e system\n",
    "    candidates = ak.combinations(electrons, 4)\n",
    "    e1, e2, e3, e4 = ak.unzip(candidates)\n",
    "    candidates[\"p4\"] = e1 + e2 + e3 + e4\n",
    "    higgs_mass = candidates[\"p4\"].mass\n",
    "    observable = ak.flatten(higgs_mass/GeV)\n",
    "    \n",
    "    return observable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select objects and events\n",
    "el, mu = object_selection(events)\n",
    "selection_4e0m = region_selection(el, mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `matplotlib` to plot a histogram of the invariant mass of each four-electron group in each event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# observable calculation and plotting\n",
    "plt.hist(calculate_inv_mass(el[selection_4e0m]).compute(), bins=50, range=(100, 150), label='Signal')\n",
    "plt.axvline(125.11, color='r', linestyle='dashed', linewidth=2, label='Expected Higgs Mass') # known only after the discovery in 2012\n",
    "plt.xlabel(r\"$m_{inv.}(4e)$ [GeV]\")\n",
    "plt.ylabel('Events')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale out over multiple files\n",
    "\n",
    "`coffea` supplies facilities for horizontally scaling an analysis in order to reduce time-to-insight in a way that is largely independent of the resource the analysis is being executed on. By making use of modern big-data technologies like `dask` it is possible with to scale an analysis from testing on a laptop to a large multi-core server, computing clusters and super-computers without the need to alter or otherwise adapt the analysis code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "# fileset = {\"ZZ\"     : {'files': {'/Users/ekourlitis/cernbox/mc20_13TeV.700600.Sh_2212_llll.deriv.DAOD_PHYSLITE.e8433_s3681_r13167_p6026/mc20_13TeV/DAOD_PHYSLITE.37110926._000047.pool.root.1' : \"CollectionTree\"},\n",
    "#                       'metadata': {'process': 'ZZ', 'xsec': 1.2973999999999999, 'nevts': 67558600}},\n",
    "#            \"Higgs\"  : {'files': {'/Users/ekourlitis/cernbox/mc20_13TeV.345060.PowhegPythia8EvtGen_NNLOPS_nnlo_30_ggH125_ZZ4l.deriv.DAOD_PHYSLITE.e7735_s3681_r13167_p6026/mc20_13TeV/DAOD_PHYSLITE.38191712._000020.pool.root.1' : \"CollectionTree\"}, \n",
    "#                       'metadata': {'process': 'Higgs', 'xsec': 28.299999999999997, 'nevts': 6500000}},}\n",
    "\n",
    "# UChicago\n",
    "fileset = {\n",
    "           \"ZZ\"     : {\n",
    "                        'files': {\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.37110937._000011.pool.root.1' : 'CollectionTree'\n",
    "                                 },\n",
    "                        'metadata': {'process': 'ZZ', 'xsec': 1.297, 'genFiltEff': 1.0, 'kFactor': 1.0, 'nevts': 2346447.75}\n",
    "                      },\n",
    "    \n",
    "           \"Higgs\"  : {\n",
    "                        'files': {\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000001.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000002.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000005.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000006.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000007.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000008.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000009.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000010.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000011.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000012.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000013.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000014.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000016.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000017.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000018.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000019.pool.root.1' : 'CollectionTree',\n",
    "                                   'root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.38191712._000020.pool.root.1' : 'CollectionTree'\n",
    "                                 },\n",
    "                        'metadata': {'process': 'Higgs', 'xsec': 28.3, 'genFiltEff': 1.240E-04, 'kFactor': 1.45, 'nevts': 114108.08}\n",
    "                      }\n",
    "          }\n",
    "\n",
    "# pre-process\n",
    "samples, _ = dataset_tools.preprocess(fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist\n",
    "import dask\n",
    "\n",
    "# create histogram with observables\n",
    "def create_histogram(events):\n",
    "    hist_4e0m = (\n",
    "        hist.dask.Hist.new.Reg(100, 100, 150, name='m_eeee', label=r\"$m_{eeee}$ [GeV]\")\n",
    "        .StrCat([], name='process', label='Process', growth=True)\n",
    "        .Weight()\n",
    "    )\n",
    "\n",
    "    process = events.metadata['process']\n",
    "\n",
    "    # normalization for MC\n",
    "    x_sec = events.metadata[\"xsec\"]\n",
    "    gen_filt_eff = events.metadata[\"genFiltEff\"]\n",
    "    k_factor = events.metadata[\"kFactor\"]\n",
    "    nevts_total = events.metadata[\"nevts\"]\n",
    "    lumi = 36000. # /pb\n",
    "\n",
    "    # calculate weights\n",
    "    if process != \"data\":\n",
    "        xsec_weight = x_sec * gen_filt_eff * k_factor * lumi / nevts_total\n",
    "    else:\n",
    "        xsec_weight = 1\n",
    "    # event_mc_weight = ak.flatten(events.EventInfo.mcEventWeights[0], axis=0)\n",
    "    # event_mc_weight = events[selection_4e0m].EventInfo.mcEventWeights[0]\n",
    "        \n",
    "    print(f\"Processing {process} with xsec weight {xsec_weight}\")\n",
    "\n",
    "    # select objects and events\n",
    "    el, mu = object_selection(events)\n",
    "    selection_4e0m = region_selection(el, mu)\n",
    "\n",
    "    # observable calculation and histogram filling\n",
    "    inv_mass = calculate_inv_mass(el[selection_4e0m])\n",
    "    hist_4e0m.fill(inv_mass, weight=xsec_weight, process=process)\n",
    "\n",
    "    return {\"4e0m\": hist_4e0m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the task graph\n",
    "tasks = dataset_tools.apply_to_fileset(create_histogram, \n",
    "                                       samples, \n",
    "                                       schemaclass=PHYSLITESchema,\n",
    "                                       uproot_options=dict(filter_name=filter_name)\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the task graph\n",
    "tasks[\"Higgs\"][\"4e0m\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# execute\n",
    "(out, ) = dask.compute(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack all the histograms together, as we processed each sample separately\n",
    "full_histogram = sum([v[\"4e0m\"] for v in out.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = full_histogram[hist.rebin(4), :].stack(\"process\").plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)\n",
    "ax.set_ylabel(\"Events\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
